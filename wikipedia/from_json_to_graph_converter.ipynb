{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data conversion from JSON to Graph"
      ],
      "metadata": {
        "id": "E5m6k19iZ7VT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0F4mlnC_m-I"
      },
      "source": [
        "Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWwxVxYT_R8c"
      },
      "outputs": [],
      "source": [
        "!pip install wikidata\n",
        "!pip install nameparser\n",
        "!pip install rdflib\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF6HvdgOCJqo"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIFRifnv_Wgf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "from nameparser import HumanName\n",
        "import requests\n",
        "from wikidata.client import Client\n",
        "from tqdm import tqdm\n",
        "import unicodedata\n",
        "import spacy\n",
        "import requests\n",
        "import re\n",
        "from rdflib import URIRef, ConjunctiveGraph, Literal, Namespace\n",
        "from rdflib.namespace import RDF, XSD, RDFS, OWL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOMqDAgnCoD_"
      },
      "source": [
        "# Data cleaning functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B626-z0n_lP6"
      },
      "source": [
        "Define a function to remove appellative from people labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0XNbuEm_lP7"
      },
      "outputs": [],
      "source": [
        "def remove_appellatives(name):\n",
        "    parsed_name = HumanName(name)\n",
        "    parsed_name.title = ''  # Remove the title\n",
        "    return str(parsed_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-EI6k3DCzaX"
      },
      "outputs": [],
      "source": [
        "def from_string_to_URI(input_string):\n",
        "    # Normalize the string to decomposed form\n",
        "    normalized_string = unicodedata.normalize('NFD', input_string)\n",
        "    # Remove diacritics\n",
        "    without_accents = ''.join(c for c in normalized_string if not unicodedata.combining(c))\n",
        "    without_spaces = without_accents.replace(\" \", \"\")\n",
        "    punctuation_chars = '''£$!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    without_punctuation = ''.join(c for c in without_spaces if c not in punctuation_chars)\n",
        "\n",
        "    return without_punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBaIhznmDjlJ"
      },
      "source": [
        "NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1jenFTbDjlS"
      },
      "outputs": [],
      "source": [
        "def extract_people_org(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ in ['ORG', 'PERSON']:\n",
        "          entities.append((ent.text, ent.label_))\n",
        "\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5OuS6hWDjlT"
      },
      "outputs": [],
      "source": [
        "def convert_name_format(full_name):\n",
        "    names = full_name.split()\n",
        "\n",
        "    if len(names) >= 2:\n",
        "        formatted_name = f\"{names[-1]}, {' '.join(names[:-1])}\"\n",
        "        return formatted_name\n",
        "    else:\n",
        "        return full_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZrlLySmCwQf"
      },
      "source": [
        "# Entity linking functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FleMcx98DjlS",
        "outputId": "cd3445b3-339a-442a-f6cf-284e36c5236b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37725281\n"
          ]
        }
      ],
      "source": [
        "def get_viaf_id_from_label(label):\n",
        "    # VIAF endpoint for searching by label\n",
        "    viaf_endpoint = \"http://www.viaf.org/viaf/AutoSuggest\"\n",
        "\n",
        "    # Parameters for the request\n",
        "    params = {\n",
        "        'query': label,\n",
        "        'sortKeys': 'holdingscount',\n",
        "        'maximumRecords': 1\n",
        "    }\n",
        "\n",
        "    # Make the request to VIAF API\n",
        "    response = requests.get(viaf_endpoint, params=params)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if 'result' in data and data['result']:\n",
        "            # Extract VIAF ID from the first result\n",
        "            viaf_id = data['result'][0]['viafid']\n",
        "            return viaf_id\n",
        "        else:\n",
        "            print(\"No results found.\")\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "\n",
        "print(get_viaf_id_from_label('Milan Šufflay'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfliKCUuIpcd"
      },
      "source": [
        "Get controlled label from VIAF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRimiuL9Iosy"
      },
      "outputs": [],
      "source": [
        "def is_latin_string(input_str):\n",
        "    latin_alphabet = set(\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")\n",
        "    if any(char in latin_alphabet for char in input_str) == True:\n",
        "      return True\n",
        "\n",
        "def get_controlled_labels(viaf_id):\n",
        "    # VIAF API endpoint for personal authority data\n",
        "    viaf_api_url = f'https://www.viaf.org/viaf/{viaf_id}/viaf.json'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(viaf_api_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "\n",
        "            controlled_labels = data['mainHeadings']['data']\n",
        "\n",
        "            if isinstance(controlled_labels, list):\n",
        "              for el in controlled_labels:\n",
        "                if is_latin_string(el['text']) == True:\n",
        "                  return el['text']\n",
        "            if isinstance(controlled_labels, dict):\n",
        "              return controlled_labels['text']\n",
        "        else:\n",
        "            print(f\"{viaf_id}, Error: Unable to retrieve labels from VIAF. Status Code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{viaf_id}, Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWgIaWfW_lP7"
      },
      "source": [
        "Define a function to retrieve wikidata IDs from labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPnNUlUK_lP7"
      },
      "outputs": [],
      "source": [
        "def get_wikidata_id_from_label(label):\n",
        "    base_url = \"https://www.wikidata.org/w/api.php\"\n",
        "\n",
        "    params = {\n",
        "        \"action\": \"wbsearchentities\",\n",
        "        \"format\": \"json\",\n",
        "        \"language\": \"en\",\n",
        "        \"search\": label\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Check if there are any results\n",
        "    if data.get(\"search\"):\n",
        "        # Return the ID of the first result\n",
        "        return data[\"search\"][0][\"id\"]\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikidata_label_from_id(wikidata_id):\n",
        "    base_url = \"https://www.wikidata.org/w/api.php\"\n",
        "\n",
        "    params = {\n",
        "        \"action\": \"wbgetentities\",\n",
        "        \"format\": \"json\",\n",
        "        \"ids\": wikidata_id,\n",
        "        \"languages\": \"en\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Check if the ID is valid and has labels\n",
        "    entity = data.get(\"entities\", {}).get(wikidata_id)\n",
        "    if entity and entity.get(\"labels\"):\n",
        "        # Return the label in the specified language (e.g., English)\n",
        "        return entity[\"labels\"][\"en\"][\"value\"]\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "jELrd_RhJO7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vg99ZQJ_lP7"
      },
      "source": [
        "Define a function to rencile textual entities to Wikidata entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9e_A9TV_lP8"
      },
      "outputs": [],
      "source": [
        "def get_entity_metadata(entity_id, prop):\n",
        "\n",
        "    client = Client()\n",
        "    entity = client.get(entity_id, load=True)\n",
        "\n",
        "    value = None\n",
        "\n",
        "    print(entity)\n",
        "\n",
        "    if prop in entity.data['claims']:\n",
        "        value = entity.data['claims'][prop][0]['mainsnak']['datavalue']['value']\n",
        "    return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQfbs1x_C-eB"
      },
      "source": [
        "# Graph generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9vQrbpJDjlT"
      },
      "outputs": [],
      "source": [
        "diz = {}\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "i, ni = 0,0\n",
        "for el in tqdm(llm_results):\n",
        "  if 'claims_best_response' in el:\n",
        "    for claim in el['claims_best_response']:\n",
        "      claim_id = el['Page ID']+str(el['claims_best_response'].index(claim))\n",
        "\n",
        "      claimer = remove_appellatives(claim['claimer_entity']).strip()\n",
        "      ner_output = extract_people_org(claimer, nlp)\n",
        "\n",
        "      # FIRST RECONCILIATION, IF THE STRING IS ALREADY CLEANED\n",
        "      if len(ner_output) == 1:\n",
        "        viaf_id =get_viaf_id_from_label(claimer)\n",
        "        wiki_id = get_wikidata_id_from_label(claimer)\n",
        "\n",
        "        temp1 = []\n",
        "        if wiki_id != None:\n",
        "          temp1.append(wiki_id)\n",
        "        if viaf_id != None:\n",
        "          temp1.append(viaf_id)\n",
        "        if len(temp1) > 0:\n",
        "          claim.update({'claimer_data':{'label':claimer, 'ids':temp1}})\n",
        "\n",
        "        # SECOND RECONCILIATION, IF THE STRING IS NOT CLEANED, BUT TRIES WITH THE NER EXTRACTED LABEL\n",
        "        if wiki_id == None and viaf_id == None:\n",
        "          # perform NER\n",
        "          ner_claimer = remove_appellatives(ner_output[0][0]).strip()\n",
        "          viaf_id =get_viaf_id_from_label(ner_claimer)\n",
        "          wiki_id = get_wikidata_id_from_label(ner_claimer)\n",
        "          if wiki_id != None and viaf_id != None:\n",
        "            temp1 = []\n",
        "          if wiki_id != None:\n",
        "            temp1.append(wiki_id)\n",
        "          if viaf_id != None:\n",
        "            temp1.append(viaf_id)\n",
        "          if len(temp1) > 0:\n",
        "            claim.update({'claimer_data':{'label':ner_claimer, 'ids':temp1}})\n",
        "\n",
        "      if len(ner_output) > 1:\n",
        "        temp = []\n",
        "        for ner in ner_output:\n",
        "          claimer =  remove_appellatives(ner[0][0]).strip()\n",
        "          wiki_id = get_wikidata_id_from_label(claimer)\n",
        "          viaf_id =get_viaf_id_from_label(claimer)\n",
        "          if wiki_id != None and viaf_id !=None:\n",
        "            if wiki_id != None:\n",
        "              temp1 = []\n",
        "            if wiki_id != None:\n",
        "              temp1.append(wiki_id)\n",
        "            if viaf_id != None:\n",
        "              temp1.append(viaf_id)\n",
        "            if len(temp1) > 0:\n",
        "              temp.append({'label':claimer, 'ids':temp1})\n",
        "        claim.update({'claimer_data':temp})\n",
        "\n",
        "print(i,ni)\n",
        "\n",
        "with open('reconciled_3_new_forgery_claims_full.json', 'w') as json_file:\n",
        "    json.dump(llm_results, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kff89cKmQcOc"
      },
      "outputs": [],
      "source": [
        "for el in llm_results:\n",
        "\n",
        "  if 'claims_best_response' in el:\n",
        "    for claim in el['claims_best_response']:\n",
        "        if 'claimer_data' not in claim:\n",
        "          print(claim[\"claimer_entity\"])\n",
        "          var = input('Do you recognise this person?')\n",
        "          if var == 'yes':\n",
        "            claim.update({'claimer_data': {\"label\": input('insert label'), \"ids\": [input('wiki id'), input('viaf id')]}})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('reconciled_3_new_forgery_claims_full.json', 'w') as json_file:\n",
        "    json.dump(llm_results, json_file, indent=4)"
      ],
      "metadata": {
        "id": "RnIqKoKS_6N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_century_string(century_str):\n",
        "    century_number = int(''.join(filter(str.isdigit, century_str)))\n",
        "    start_year = (century_number - 1) * 100 + 1\n",
        "    end_year = century_number * 100\n",
        "\n",
        "    start_date = datetime(start_year, 1, 1)\n",
        "    end_date = datetime(end_year, 12, 31)\n",
        "\n",
        "    return start_date, end_date"
      ],
      "metadata": {
        "id": "3PdwQg137PZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dateutil import parser\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def timespan_handling(date_string, nlp):\n",
        "\n",
        "  doc = nlp(date_string)\n",
        "  entities = []\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ in ['DATE']:\n",
        "        entities.append((ent.text, ent.label_))\n",
        "\n",
        "    for date_string in entities:\n",
        "      try:\n",
        "          parsed_date = parser.parse(date_string[0])\n",
        "          print(f\"Original: {date_string}, Parsed: {parsed_date}\")\n",
        "          input = input('Is the date correct, Y or N')\n",
        "          if input == 'Y':\n",
        "            return parsed_date\n",
        "          else:\n",
        "            temp_date = input('Print timespan begin and end')\n",
        "      except ValueError:\n",
        "          print(f\"Unable to parse: {date_string}\")\n",
        "          temp_date = input('Print timespan begin and end')\n",
        "\n",
        "  # handle centuries\n",
        "  else:\n",
        "    try:\n",
        "      parsed_date = parse_century_string(date_string)\n",
        "      input = input('Is the date correct, Y or N')\n",
        "      if input == 'Y':\n",
        "        return parsed_date\n",
        "      else:\n",
        "        temp_date = input('Print timespan begin and end')\n",
        "      return parsed_date\n",
        "    except:\n",
        "      print(f\"Unable to parse century: {date_string}\")\n",
        "      temp_date = input('Print timespan begin and end')"
      ],
      "metadata": {
        "id": "KAc3jGtc3fRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def geonames_api_request(place_name):\n",
        "    base_url = \"http://api.geonames.org/search?\"\n",
        "    params = {\n",
        "        'q': place_name,\n",
        "        'username': 'demo',  # Replace with your GeoNames username\n",
        "        'type': 'json',  # You can change the response format if needed\n",
        "        'fuzzy':0.8\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        print(data)\n",
        "        return data\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "Qg-hiSkHAZ3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0pZGGggDAw6"
      },
      "outputs": [],
      "source": [
        "g = ConjunctiveGraph()\n",
        "\n",
        "FORGONT = Namespace(\"http://www.example.org/\")\n",
        "g.bind(\"forgont\", FORGONT)\n",
        "\n",
        "VIAF = Namespace(\"https://viaf.org/viaf/\")\n",
        "g.bind(\"viaf\", VIAF)\n",
        "\n",
        "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
        "g.bind(\"wd\", WD)\n",
        "\n",
        "DCT = Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
        "g.bind(\"dct\", DCT)\n",
        "\n",
        "for el in llm_results:\n",
        "\n",
        "  #document URI\n",
        "  if \"Document metadata\" in el and len(el[\"Document metadata\"][0]) > 0:\n",
        "    document_ID = from_string_to_URI(el[\"Document metadata\"][0][\"document_title\"].strip())\n",
        "  else:\n",
        "      document_ID = from_string_to_URI(el['Page URL'].replace('https://en.wikipedia.org/wiki/', '').replace('\\n', '').strip())\n",
        "  document_URI = URIRef(FORGONT+document_ID)\n",
        "  alleged_context = URIRef(FORGONT+document_ID+'_doc_claim')\n",
        "\n",
        "  if \"Document metadata\" in el:\n",
        "    if len(el[\"Document metadata\"][0]) > 0:\n",
        "      g.add((document_URI, DCT.title, Literal(el[\"Document metadata\"][0][\"document_title\"], datatype=XSD.string)))\n",
        "      g.add((document_URI, RDF.type, URIRef(FORGONT+from_string_to_URI(el[\"Document metadata\"][0][\"document_support\"])), alleged_context))\n",
        "      #timespan = timespan_handling(el[\"Document metadata\"][0]['document_alleged_date'], nlp)\n",
        "      # g.add((document_URI, DCT.date, Literal(el[\"Document metadata\"][0]['document_alleged_date'], datatype=XSD.string), alleged_context))\n",
        "      #g.add((document_URI, DCT.place, Literal(el[\"Document metadata\"][0]['document_alleged_place'], datatype=XSD.string), alleged_context))\n",
        "      geo_id = geonames_api_request(el[\"Document metadata\"][0]['document_alleged_place'])\n",
        "      g.add((document_URI, DCT.creator, Literal(el[\"Document metadata\"][0]['document_alleged_creator'], datatype=XSD.string), alleged_context))\n",
        "\n",
        "  if 'claims_best_response' in el:\n",
        "    for claim in el['claims_best_response']:\n",
        "\n",
        "      # CLAIM CONTEXTUAL INFORMATION\n",
        "      if 'claimer_data' in claim:\n",
        "\n",
        "        # IF ONLY ONE AUTHOR IS DETECTED IN THE CLAIM\n",
        "        if isinstance(claim.get('claimer_data'), dict):\n",
        "          if len(claim['claimer_data'][\"ids\"]) > 0:\n",
        "            wiki = [id_value for id_value in claim['claimer_data'][\"ids\"] if id_value.startswith(\"Q\")]\n",
        "            viaf = [id_value for id_value in claim['claimer_data'][\"ids\"] if not id_value.startswith(\"Q\")]\n",
        "\n",
        "            # IF VIAF ID FOR CLAIM AUHTOR, ADD THE CLAIM - all claims made by the same person/org on the same document are grouped in the same Named Graph\n",
        "            if len(viaf) > 0:\n",
        "              context = URIRef(FORGONT+el['Page URL'].replace('https://en.wikipedia.org/wiki/','')+from_string_to_URI(viaf[0]))\n",
        "              claim_author_URI = URIRef(FORGONT+from_string_to_URI(viaf[0]))\n",
        "              g.add((claim_author_URI, RDF.type, FORGONT.Person))\n",
        "              g.add((claim_author_URI, RDFS.label, Literal(get_controlled_labels(viaf[0]), datatype=XSD.string)))\n",
        "              g.add((claim_author_URI, OWL.sameAs, URIRef(VIAF+viaf[0])))\n",
        "              g.add((context, FORGONT.author, claim_author_URI))\n",
        "              g.add((context, RDFS.comment, Literal(claim['claim_text'], datatype=XSD.string)))\n",
        "              # claim's opinion\n",
        "              claim_opinion_URI = URIRef(FORGONT+claim['claim_opinion'])\n",
        "              g.add((document_URI, RDF.type, claim_opinion_URI, context))\n",
        "              if len(wiki) > 0:\n",
        "                g.add((claim_author_URI, OWL.sameAs, URIRef(WD+wiki[0])))\n",
        "\n",
        "            # IF NO VIAF BUT A WIKIDATA ID, ADD THE CLAIM - all claims made by the same person/org on the same document are grouped in the same Named Graph\n",
        "            elif len(viaf) == 0 and len(wiki) > 0:\n",
        "              context = URIRef(FORGONT+el['Page URL'].replace('https://en.wikipedia.org/wiki/','')+from_string_to_URI(wiki[0]))\n",
        "              claim_author_URI = URIRef(FORGONT+from_string_to_URI(wiki[0]))\n",
        "              g.add((claim_author_URI, RDF.type, FORGONT.Person))\n",
        "              g.add((claim_author_URI, RDFS.label, Literal(get_wikidata_label_from_id(wiki[0]), datatype=XSD.string)))\n",
        "              g.add((claim_author_URI, OWL.sameAs, URIRef(WD+wiki[0])))\n",
        "              g.add((context, FORGONT.author, claim_author_URI))\n",
        "              g.add((context, RDFS.comment, Literal(claim['claim_text'], datatype=XSD.string)))\n",
        "              # claim's opinion\n",
        "              claim_opinion_URI = URIRef(FORGONT+claim['claim_opinion'])\n",
        "              g.add((document_URI, RDF.type, claim_opinion_URI, context))\n",
        "          else:\n",
        "              # TO DO\n",
        "              pass\n",
        "\n",
        "        # IF MULTILPLE AUHTORS ARE DETECTED IN THE CLAIM\n",
        "        if isinstance(claim.get('claimer_data'), list):\n",
        "          for author in claim['claimer_data']:\n",
        "            wiki = [id_value for id_value in author[\"ids\"] if id_value.startswith(\"Q\")]\n",
        "            viaf = [id_value for id_value in author[\"ids\"] if not id_value.startswith(\"Q\")]\n",
        "\n",
        "            # IF VIAF ID FOR CLAIM AUHTOR, ADD THE CLAIM - all claims made by the same person/org on the same document are grouped in the same Named Graph\n",
        "            if len(viaf) > 0:\n",
        "              context = URIRef(FORGONT+el['Page URL'].replace('https://en.wikipedia.org/wiki/','').strip()+from_string_to_URI(viaf[0]))\n",
        "              claim_author_URI = URIRef(FORGONT+viaf[0].strip())\n",
        "              g.add((claim_author_URI, RDF.type, FORGONT.Person))\n",
        "              g.add((claim_author_URI, RDFS.label, Literal(get_controlled_labels(viaf[0]), datatype=XSD.string)))\n",
        "              g.add((claim_author_URI, OWL.sameAs, URIRef(VIAF+viaf[0])))\n",
        "              g.add((context, FORGONT.author, claim_author_URI))\n",
        "              # claim's opinion\n",
        "              claim_opinion_URI = URIRef(FORGONT+claim['claim_opinion'])\n",
        "              g.add((document_URI, RDF.type, claim_opinion_URI, context))\n",
        "              g.add((context, RDFS.comment, Literal(claim['claim_text'], datatype=XSD.string)))\n",
        "              if len(wiki) > 0:\n",
        "                g.add((claim_author_URI, OWL.sameAs, URIRef(WD+wiki[0])))\n",
        "\n",
        "            # IF NO VIAF BUT A WIKIDATA ID, ADD THE CLAIM - all claims made by the same person/org on the same document are grouped in the same Named Graph\n",
        "            elif len(viaf) == 0 and len(wiki) > 0:\n",
        "              context = URIRef(FORGONT+el['Page URL'].replace('https://en.wikipedia.org/wiki/','').strip()+from_string_to_URI(viaf[0]))\n",
        "              claim_author_URI = URIRef(FORGONT+from_string_to_URI(wiki[0]))\n",
        "              g.add((claim_author_URI, RDF.type, FORGONT.Person))\n",
        "              g.add((claim_author_URI, RDFS.label, Literal(get_wikidata_label_from_id(wiki[0]), datatype=XSD.string)))\n",
        "              g.add((claim_author_URI, OWL.sameAs, URIRef(WD+wiki[0])))\n",
        "              g.add((context, FORGONT.author, claim_author_URI))\n",
        "              # claim's opinion\n",
        "              claim_opinion_URI = URIRef(FORGONT+claim['claim_opinion'])\n",
        "              g.add((document_URI, RDF.type, claim_opinion_URI, context))\n",
        "              g.add((context, RDFS.comment, Literal(claim['claim_text'], datatype=XSD.string)))\n",
        "\n",
        "g.serialize(destination='prova.trig', format=\"trig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "mx33K3XmaysG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example SPARQL query\n",
        "query = \"\"\"\n",
        "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "    PREFIX forgont: <http://www.example.org/>\n",
        "\n",
        "    SELECT DISTINCT ?subject (COUNT(distinct ?g) as ?n)\n",
        "    WHERE {\n",
        "\n",
        "    VALUES ?class { forgont:Authentic forgont:Suspicious forgont:Forgery }\n",
        "        GRAPH ?g {?subject a ?class}\n",
        "    }\n",
        "\n",
        "    GROUP BY ?subject ORDER BY DESC (?n)\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "results = g.query(query)\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(\"Subject:\", row.subject)\n",
        "    print(\"n:\", row.n)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnVmhdCivuxF",
        "outputId": "3292773b-9b5b-4e80-87a3-dcc0116262fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: http://www.example.org/HistoriaAugusta\n",
            "n: 14\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/IrelandShakespeareManuscripts\n",
            "n: 13\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheDvurKraloveandZelenaHoramanuscripts\n",
            "n: 12\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/DonationofConstantine\n",
            "n: 12\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/CharterofDukeTrpimir\n",
            "n: 10\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/PactaConventaCroatia\n",
            "n: 9\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheCorrespondenceofPaulandSeneca\n",
            "n: 9\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/AlthochdeutschesSchlummerlied\n",
            "n: 8\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/DeSituBritanniae\n",
            "n: 8\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheSalamanderLetter\n",
            "n: 7\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/AshtinameofMuhammad\n",
            "n: 6\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/CodeofRajahKalantiaw\n",
            "n: 6\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/MussoliniDiaries\n",
            "n: 6\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheFranklinProphecy\n",
            "n: 6\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/OathofaFreeman\n",
            "n: 6\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheReportfromIronMountain\n",
            "n: 5\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/PataKhazana\n",
            "n: 5\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheProtocolsoftheEldersofZion\n",
            "n: 5\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheMarriageCharterofEmpressTheophanu\n",
            "n: 5\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheBookofVeles\n",
            "n: 5\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/AnIncompleteHistoryoftheArtoftheFuneraryViolin\n",
            "n: 5\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/Codex2427\n",
            "n: 5\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/AProtocolof1919\n",
            "n: 4\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheEnglishMercurie\n",
            "n: 4\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheHitlerDiaries\n",
            "n: 4\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheChronicleofRivius\n",
            "n: 4\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/AmadoCrowley\n",
            "n: 4\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheVinlandMap\n",
            "n: 4\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/BrainWashingASynthesisoftheRussianTextbookonPsychopolitics\n",
            "n: 3\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/CryingWind\n",
            "n: 3\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/MySisterandI\n",
            "n: 3\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/MathildeLefebvreletter\n",
            "n: 3\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheChroniclesofEri\n",
            "n: 3\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/ZenoMapandLetters\n",
            "n: 3\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/Pamietnikhandlowca\n",
            "n: 3\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/WilliamLynchSpeech\n",
            "n: 3\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheDarkTower\n",
            "n: 2\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/Kujiki\n",
            "n: 2\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheWillofPetertheGreat\n",
            "n: 2\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/HotsumaTsutae\n",
            "n: 2\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheBookofJasher\n",
            "n: 2\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/SouvenirsdelaMarquisedeCrequy\n",
            "n: 2\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/Document125713570\n",
            "n: 2\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/ClotildedeSurvillepoems\n",
            "n: 2\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/HistoriasdelaConquistadelMayab\n",
            "n: 1\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/TheLetterofBenan\n",
            "n: 1\n",
            "\n",
            "\n",
            "Subject: http://www.example.org/Privilegiummaius\n",
            "n: 1\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Un benvenuto a Colaboratory",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
