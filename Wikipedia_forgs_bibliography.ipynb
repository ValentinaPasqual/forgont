{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "document_claims, document_alignment = {}, {}\n",
        "\n",
        "# Open the CSV file\n",
        "with open('wikipedia_forgeries_corpus.csv', 'r', encoding='utf-8') as file:\n",
        "    # Create a CSV reader\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    # Iterate over each row in the CSV\n",
        "    for row in reader:\n",
        "        # Access the \"Document claims\" column and process its content\n",
        "        document_claims.update({row['Page URL']:row['Document claims']})\n",
        "\n",
        "        document_alignment.update({row['Page URL']:row['Page ID']})"
      ],
      "metadata": {
        "id": "Fqp_3SVGOcwL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_references(url):\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url.strip())\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the references section by its class\n",
        "        references_section = soup.find_all('div', {'class': 'reflist'})\n",
        "\n",
        "        references_section = references_section[0]\n",
        "\n",
        "        # Initialize a dictionary to store references and their numbers\n",
        "        references = {}\n",
        "\n",
        "\n",
        "        # Extract text and references numbers from the references section\n",
        "        if references_section:\n",
        "            ref_tags = references_section.find_all('span', class_='reference-text')\n",
        "\n",
        "            for index, tag in enumerate(ref_tags, start=1):\n",
        "                ref_number = f\"[{index}]\"\n",
        "\n",
        "                ref_text = tag.get_text(separator=' ', strip=True)\n",
        "\n",
        "                links = tag.find_all('a')\n",
        "                boolean = False\n",
        "                try:\n",
        "                  for link in links:\n",
        "                    ref_id = link.get('href').replace('#', '')\n",
        "                    bib = soup.find('cite', {'id': ref_id})\n",
        "                    if bib != None:\n",
        "                      references[ref_number] = bib.get_text(separator=' ', strip=True)\n",
        "                      boolean = True\n",
        "\n",
        "                  if boolean == False:\n",
        "                    references[ref_number] = ref_text\n",
        "\n",
        "                except:\n",
        "                    references[ref_number] = ref_text\n",
        "\n",
        "            return references\n",
        "        else:\n",
        "            print(\"References section not found at\", url)\n",
        "            return \"References section not found.\"\n",
        "    else:\n",
        "        print(\"Failed to fetch Wikipedia page.\")\n",
        "        return \"Failed to fetch Wikipedia page.\""
      ],
      "metadata": {
        "id": "8DeUXoefvdsf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "pattern = r'\\[\\d+\\]'\n",
        "\n",
        "with open('references.tsv', 'w', newline='', encoding='utf-8') as tsvfile:\n",
        "\n",
        "    # Define TSV writer\n",
        "    writer = csv.DictWriter(tsvfile, delimiter='\\t', fieldnames=['Page ID', 'Document URL', 'Reference number', 'Reference'])\n",
        "    writer.writeheader()\n",
        "\n",
        "\n",
        "    for url, text in document_claims.items():\n",
        "\n",
        "        # Find all matches\n",
        "        citations = re.findall(pattern, text)\n",
        "        citations = list(set(citations))\n",
        "\n",
        "        # Get the references section of the Wikipedia page\n",
        "        references = get_references(url)\n",
        "\n",
        "        # Write data to CSV\n",
        "        ref_texts = {}\n",
        "\n",
        "        if isinstance(references, dict):\n",
        "\n",
        "            if len(references) == 0:\n",
        "              print(f'No references found in {url}')\n",
        "\n",
        "            # Iterate through references\n",
        "            for ref_number, ref_text in references.items():\n",
        "                # Check if the reference number is in the citations\n",
        "                if ref_number in citations:\n",
        "                    # If the reference text is already encountered, append the reference number\n",
        "                    if ref_text in ref_texts:\n",
        "                        ref_texts[ref_text].append(ref_number)\n",
        "                    else:\n",
        "                        ref_texts[ref_text] = [ref_number]\n",
        "\n",
        "            # Write the data to CSV (moved outside the loop over references)\n",
        "            for ref_text, ref_numbers in ref_texts.items():\n",
        "                for wiki_url, wiki_id in document_alignment.items():\n",
        "                    if url == wiki_url:\n",
        "                      writer.writerow({'Page ID': wiki_id, 'Document URL': url, 'Reference number': ','.join(ref_numbers), 'Reference': ref_text})"
      ],
      "metadata": {
        "id": "i8nCELCyxqfy"
      },
      "execution_count": 33,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}